---
layout: post
title: "Text mining and Natural Language Processing (NLP)"
subtitle: "Programming language used: R"
background: '/img/posts/nlp/nodes.png'
---

## Text analysis of an ancient Korean language 

This is a sample of my freelance data work for a Hanyang University Ph.D student in 2018 and 2019. I asked for a permission to upload some of my report, and he generously allowed me to do so. My client wanted to read the trend of the era through old newspapers published from the 18th century to the early 19th century. The project took about seven months because of the difficulty of analyzing ancient scripts that are no longer in use.

I analysed a total of **5 old newspapers**, and the following report is a part of the analysis of one of the newspapers. The newspaper was published everyday from *1896 to 1899*. The main tasks I performed were as follows.

- Programmed a crawling software for text mining
- Pre-processed the corpus by buidling a bag-of-words model
- Analyses and visualizations

Here is a list of the analyses I conducted: 
- Frequency based text analysis
- DTM(document-term matrix), TDM(term-document matrix), and covariance matrix analysis
- Co-occuring word analysis
- Network analysis
- TF-IDF(term frequency-inverse document frequency) measure
- Linkage analysis
- Topic modelling

The images below are screenshots from the reports I submitted to my client. The codes and the explanation of them were also attached to the report because the client wanted to be able to replicate the codes himself. Therefore, the submitted report was consited of two parts: the analysis itself and the explanation of the analysis method and coding. This project was challenging and rewarding at the same time becuase I had to explain every steps in layman's terms.  


### Programming a crawling software
![DR1](\img\posts\nlp\dr1_1_1.png)

 First, I had to extract and save all of the articles from the newspaper. Using R, I coded a program which scraps data from Korean Newspaper Archive. The program automatically gathered the articles and saved them by date. Due to the large volume of documents, it took several days just to save the data. 

### Pre-processing the corpus by buidling a bag-of-words model
![DR2](\img\posts\nlp\dr2_1.png)

Usually pre-processing takes up most of the text analysis work. It was especially difficult because it was necessary to convert the ancient Korean into a modern language. In the pre-processing process, characters that are not needed for analysis are deleted.

![DR2](\img\posts\nlp\dr2_2.png)

Since advertisements in newspapers often appear multiple times, duplicate articles are deleted. Pre-processing is a series of tedious and detailed work. Since there are differences between the modern spacing rule and the old spacing rule, I had to build a dictionary for it. Using this dictionary I applied the new spacing rule on the text. 

![DR3](\img\posts\nlp\dr3_1.png)

The next task is word extraction. In order to perform the analysis that my client wants, the text had be transformed into a bag-of-words model. However, since there are many word expressions and grammar that are no longer used in modern Korean, it was necessary to manually register those expressions. I was not an expert of ancient Korean grammar, so I had to have several meetings with my client at this point. 

![DR4](\img\posts\nlp\dr4_1.png)

I also had to build a dictinary for stop words. Stop words are a set of commonly used words in a language. For example, in English, articles such as “a” or “the” could be a stop word. My stop words dictinary included old Korean words which are commonly used but cary very little useful information. These words were eliminated using the dictionary. There are many other pre-processing tasks that I had to deal with, but I won't list them all here.

![DR4](\img\posts\nlp\dr4_2.png)

After all the pre-processing works text data were transformed into corpora. 

### Frequency based text analysis
![DR5](\img\posts\nlp\dr5_1.png)

Finally the real fun began. Before going into the detailed analyses, a frequency analysis was performed to see the overall picture.

![DR6](\img\posts\nlp\dr6_1.png)

Based on the corpus created, the absolute frequency was calculated in descending order and saved as a data frame. This was visualized using word cloud and frequency chart. For curious non-Korean readers, the most frequent word was 'people', followed by 'government', 'Japan' and 'country'. Considering the historical situation of Korea at the time, it is not surprising that Japan appeared many times in newspaper.

### DTM, TDM, and covariance matrix analysis

![DR7](\img\posts\nlp\dr7_1.png)

For further analyses, the corpus was converted to DTM and TDM. The document-term matrix (DTM), which is a document * word matrix, and the term-document matrix (TDM), which is a word * document matrix, both represent the frequency of occurrence of a specific word in a specific document as a matrix.



###  Co-occuring word analysis
![DR8](\img\posts\nlp\dr8.png)
![DR9](\img\posts\nlp\dr9.png)
### Topic modelling
![DR10](\img\posts\nlp\dr10.png)
![DR10](\img\posts\nlp\dr11.png)
### Network analysis
![DR10](\img\posts\nlp\dr12.png)
![DR10](\img\posts\nlp\dr13.png)
![DR10](\img\posts\nlp\dr14.png)
### TF-IDF measure

![DR10](\img\posts\nlp\dr15.png)

### Linkage analysis
![DR10](\img\posts\nlp\dr16.png)
![DR10](\img\posts\nlp\dr17.png)